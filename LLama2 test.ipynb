{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import ChatCompletionMessage, Llama\n",
    "\n",
    "path = \"C:/Users/georg/AppData/Local/llama_index/models/llama-2-13b-chat.ggmlv3.q4_0.bin\"\n",
    "llm = Llama(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__del__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_candidates',\n",
       " '_candidates_data',\n",
       " '_candidates_data_id',\n",
       " '_candidates_data_p',\n",
       " '_convert_text_completion_chunks_to_chat',\n",
       " '_convert_text_completion_to_chat',\n",
       " '_create_completion',\n",
       " '_input_ids',\n",
       " '_n_ctx',\n",
       " '_n_vocab',\n",
       " '_p_tensor_split',\n",
       " '_sample',\n",
       " '_scores',\n",
       " '_token_eos',\n",
       " '_token_nl',\n",
       " 'cache',\n",
       " 'create_chat_completion',\n",
       " 'create_completion',\n",
       " 'create_embedding',\n",
       " 'ctx',\n",
       " 'detokenize',\n",
       " 'embed',\n",
       " 'eval',\n",
       " 'eval_logits',\n",
       " 'eval_tokens',\n",
       " 'generate',\n",
       " 'input_ids',\n",
       " 'last_n_tokens_size',\n",
       " 'load_state',\n",
       " 'logits_to_logprobs',\n",
       " 'longest_token_prefix',\n",
       " 'lora_base',\n",
       " 'lora_path',\n",
       " 'model',\n",
       " 'model_path',\n",
       " 'n_batch',\n",
       " 'n_ctx',\n",
       " 'n_embd',\n",
       " 'n_parts',\n",
       " 'n_threads',\n",
       " 'n_tokens',\n",
       " 'n_vocab',\n",
       " 'params',\n",
       " 'reset',\n",
       " 'sample',\n",
       " 'save_state',\n",
       " 'scores',\n",
       " 'set_cache',\n",
       " 'tensor_split',\n",
       " 'token_bos',\n",
       " 'token_eos',\n",
       " 'token_nl',\n",
       " 'tokenize',\n",
       " 'tokenizer',\n",
       " 'verbose']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current CE\n",
      "[{'text': '\\nThe current CE', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
     ]
    }
   ],
   "source": [
    "def generate_text(\n",
    "    prompt=\"Who is the CEO of Apple?\",\n",
    "    max_tokens=32,\n",
    "    temperature=0,\n",
    "    top_p=0.5,\n",
    "    echo=False,\n",
    "    stop=[\"#\"],\n",
    "):\n",
    "    output = llm(\n",
    "        prompt,\n",
    "        max_tokens=4,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        echo=echo,\n",
    "        stop=stop,\n",
    "    )\n",
    "    output_text = output[\"choices\"][0][\"text\"].strip()\n",
    "    return output_text, output\n",
    "\n",
    "output_text, output = generate_text(\"Who is the CEO of Apple?\")\n",
    "print(output_text)\n",
    "print(output[\"choices\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
