{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import re\n",
    "import string\n",
    "\n",
    "#model obtained from https://gpt4all.io/index.html?s=09\n",
    "path = \"C:/Users/georg/Downloads/orca-mini-3b.ggmlv3.q4_0.bin\"\n",
    "llm = Llama(path, logits_all = True, n_gpu_layers=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#privateChars is a list of 10 unicode private characters which have no universal interpretation and as such using these will not interrupt standard strings \n",
    "privateChars = [\"\\uE000\", \"\\uE001\", \"\\uE002\", \"\\uE003\", \"\\uE004\", \"\\uE005\", \"\\uE006\", \"\\uE007\", \"\\uE008\", \"\\uE009\"]\n",
    "\n",
    "def generateText(prompt, tokens = 1, temperature = 0, topP = 0.5, echo = False,stop = [\"#\"]):\n",
    "    #generateText is a wrapped function which predicts the next set of tokens using a predefined llm\n",
    "    output = llm(prompt, max_tokens = tokens, temperature = temperature, top_p = topP, \n",
    "                 echo=echo, stop=stop, logprobs = 10)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def removeSpaces(inputString):\n",
    "    #removeSpaces is a wrapper function to remove all spaces from a string\n",
    "    translator = str.maketrans('', '', \" \") \n",
    "    return inputString.translate(translator)\n",
    "\n",
    "def encode(plain, searchLength = 10):\n",
    "    #split the plain text string into a list of individual words\n",
    "    words = plain.split()\n",
    "    \n",
    "    #iterate backwards through words in order to encode incrementally\n",
    "    for i, word in reversed(list(enumerate(words))):\n",
    "\n",
    "        #get the searchLength words before the target string and create a string of them\n",
    "        startInd = max(0, i - searchLength)\n",
    "        plainSection = \" \".join(words[startInd: i])\n",
    "        \n",
    "        #use generateText to get a dictionary of the predictions of the next word from the llm\n",
    "        output = generateText(plainSection)\n",
    "\n",
    "        #remove all spaces from the target string\n",
    "        word = removeSpaces(word)\n",
    "\n",
    "        try:\n",
    "            #reformat the dictionary of the predictions to have all spaces removed\n",
    "            choiceDict = output[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0]\n",
    "            choiceDict  = {removeSpaces(key) : value for key, value in choiceDict.items()}\n",
    "        except:\n",
    "            #if the predictions fail then an empty dictionary will be returned\n",
    "            choiceDict = {}\n",
    "\n",
    "        #check to see if the target string is one of the predicted strings and if so replace the string with the corresponding \n",
    "        #unicode private character, where the n th most likely word is replaced with the n th private character\n",
    "        if word in choiceDict:\n",
    "            keys = list(choiceDict.keys())\n",
    "            words[i] = privateChars[keys.index(word)]\n",
    "\n",
    "    #rejoin the words into text\n",
    "    return \" \".join(words)\n",
    "\n",
    "def decode(encoded, searchLength = 10):\n",
    "    #split the encoded string into a list of individual words\n",
    "    words = encoded.split()\n",
    "\n",
    "    #create a pattern which finds all the private characters in the list words and gets their locations\n",
    "    pattern = \"|\".join(re.escape(char) for char in privateChars)\n",
    "    preds  = [i for i, s in enumerate(words) if re.search(pattern, s)]\n",
    "    \n",
    "    #itterate over all of the encoded words\n",
    "    for pred in preds:\n",
    "\n",
    "        #get the searchLength words before the encoded word and create a string of them\n",
    "        startInd = max(0, pred - searchLength)\n",
    "        plainSection = \" \".join(words[startInd: pred])\n",
    "\n",
    "        #use generateText to get a dictionary of the predictions of the unencoded word from the llm\n",
    "        output = generateText(plainSection)\n",
    "\n",
    "        #reformat the dictionary of the predictions to have all spaces removed\n",
    "        choiceDict = output[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0]\n",
    "        choiceDict = {removeSpaces(key) : value for key, value in choiceDict.items()}\n",
    "        \n",
    "        #get a list of the most likely options for the encoded word \n",
    "        keys = list(choiceDict.keys())\n",
    "\n",
    "        #replace the private character for the corresponding prediction\n",
    "        wordChoice = privateChars.index(words[pred])\n",
    "        words[pred] = keys[wordChoice].strip()\n",
    "\n",
    "    #rejoin the words into text  \n",
    "    return \" \".join(words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded correctly\n"
     ]
    }
   ],
   "source": [
    "#This is a unit test to check the encoding and decoding is lossless. This not put in a standard unit test file as the llm is very computationally expensive\n",
    "testPlain = \"A large language model (LLM) is a language model characterized by its large size. Their size, which could be as large as 180 billion parameters, is enabled by AI accelerators, which are able to process vast amounts of text data, mostly scraped from the Internet.\"\n",
    "testEncoded = encode(testPlain, 5)\n",
    "testDecoded = decode(testEncoded, 5)\n",
    "\n",
    "assert testPlain == testDecoded\n",
    "print(\"Decoded correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to encode: 92.63611102104187\n",
      "A large language  (LLM)   language  characterized    size. Their size,    as   180  parameters,  enabled  AI accelerators,   able      text data, mostly scraped   Internet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to decode: 51.388925075531006\n",
      "A large language model (LLM) is a language model characterized by its large size. Their size, which could be as large as 180 billion parameters, is enabled by AI accelerators, which are able to process vast amounts of text data, mostly scraped from the Internet.\n",
      "Decoded correctly\n",
      "0.7480916030534351\n"
     ]
    }
   ],
   "source": [
    "#This is an example of loading a text file and then encoding and decoding it\n",
    "#I have found this process to be very time consuming and leads to a 20 to 30 percent reduction in string length\n",
    "f = open(\"toEncode.txt\")\n",
    "plain = f.read()\n",
    "f.close()\n",
    "plain = plain.replace(\"\\n\", \"\")\n",
    "\n",
    "encoded = encode(plain, 10)\n",
    "print(encoded)\n",
    "decoded = decode(encoded, 10)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35312902593339444703742\n",
      "The best reality television of recent times is , which has made me appreciate and appreciate real stories more and to be a real life hero\n",
      "35312902593339444703742\n"
     ]
    }
   ],
   "source": [
    "#this is another usecase i thought of after starting the project\n",
    "#convert the code number to the correspoding private character and then turn that into a string\n",
    "codeNumber = 35312902593339444703742\n",
    "codeStr = str(codeNumber)\n",
    "codeStr = [privateChars[int(codeChar)] for codeChar in codeStr]\n",
    "codeStr = \" \".join(codeStr)\n",
    "\n",
    "#add the context string to the start of the code string and then decode the string to the cipher text\n",
    "contextStr = \"The best reality \"\n",
    "contextLen = len(contextStr.split())\n",
    "cipherStr = decode(contextStr + codeStr, 20)\n",
    "\n",
    "#using the same llm settings unencrypt the string and then return the codeNumber from before\n",
    "decodedChar = encode(cipherStr, 20).split()[contextLen: ]\n",
    "decodedNum = [privateChars.index(char) for char in decodedChar]\n",
    "decodedNumber = int(''.join(map(str, decodedNum)))\n",
    "\n",
    "print(codeNumber)\n",
    "print(cipherStr)\n",
    "print(decodedNumber)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
